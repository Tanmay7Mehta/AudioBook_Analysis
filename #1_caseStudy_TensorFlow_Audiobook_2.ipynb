{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "aff42577-f7a8-4455-bc0a-e479a3151ded",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n",
    "\n",
    "unscaled_input_all = raw_csv_data[:, 1:-1]\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "64ca6cd6-714d-41d2-880a-dc8527eed644",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "#num_one_targets\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_input_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "481096c1-b234-46f8-9823-a1658ddc92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9d80bea1-ebbf-43aa-941f-8d5ddf5f3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "5f4896b9-bfc8-4edd-99fc-b6e11ec6aabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1785.0 3579 0.49874266554903607\n",
      "230.0 447 0.5145413870246085\n",
      "222.0 448 0.4955357142857143\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count: train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count: train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets)/train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets)/validation_samples_count)\n",
    "print(np.sum(test_targets),test_samples_count, np.sum(test_targets)/test_samples_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "eb3a1592-e885-4c43-859d-a31fbd71c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobook_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobook_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobook_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d758c7-6a34-4751-bc29-6b58e85043a0",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "fcaa71f8-f9c5-4458-8bd4-5b7b7127cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class that will do the batching for the algorithm\n",
    "class Audiobooks_Data_Reader():\n",
    "    # Dataset is a mendatory argurment, while batch_size is optional.\n",
    "    # If you dont input batch_size, it will automatically take the value: None\n",
    "    def __init__(self, dataset, batch_size=None):\n",
    "        \n",
    "        # The dataset that loads is one of 'train', 'validation', 'test'\n",
    "        # eg-> if i call this class with x('train', 5), it will load 'Audiobooks_data_train.npz' with a batch size of 5.\n",
    "        npz = np.load('Audiobook_data_{0}.npz'.format(dataset))\n",
    "        self.inputs, self.targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
    "        if batch_size is None:\n",
    "            self.batch_size = self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.curr_batch >= self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "\n",
    "        # You slice the dataset in batches and then the \"next\" function loads them one after the other.\n",
    "        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch = self.targets[batch_slice]\n",
    "        self.curr_batch += 1\n",
    "\n",
    "        # One-hot encode the targets.\n",
    "        classes_num = 2\n",
    "        #targets_one_hot = np.zeros((targets_batch.size[0], classes_num))\n",
    "        #targets_one_hot[range(targets_batch.size[0]), targets_batch] = 1\n",
    "        targets_one_hot = np.zeros((len(targets_batch), classes_num))\n",
    "        targets_one_hot[np.arange(len(targets_batch)), targets_batch] = 1\n",
    "\n",
    "        return inputs_batch, targets_one_hot\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ced674-0608-4743-b3ef-73546ff09afa",
   "metadata": {},
   "source": [
    "### Machine Learning Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c4eb17df-3e30-4392-a84d-4ecff95fcd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "5e4cb963-de2a-4764-a9b3-52a182be38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10may\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:1793: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1. Training loss:  1.254. Validation loss:  0.732. Validation accuracy 51.454%\n",
      "Epoch2. Training loss:  0.648. Validation loss:  0.601. Validation accuracy 61.298%\n",
      "Epoch3. Training loss:  0.554. Validation loss:  0.545. Validation accuracy 69.575%\n",
      "Epoch4. Training loss:  0.498. Validation loss:  0.503. Validation accuracy 73.154%\n",
      "Epoch5. Training loss:  0.455. Validation loss:  0.472. Validation accuracy 74.497%\n",
      "Epoch6. Training loss:  0.423. Validation loss:  0.447. Validation accuracy 76.510%\n",
      "Epoch7. Training loss:  0.400. Validation loss:  0.429. Validation accuracy 78.076%\n",
      "Epoch8. Training loss:  0.383. Validation loss:  0.418. Validation accuracy 78.076%\n",
      "Epoch9. Training loss:  0.371. Validation loss:  0.409. Validation accuracy 78.076%\n",
      "Epoch10. Training loss:  0.362. Validation loss:  0.401. Validation accuracy 78.523%\n",
      "Epoch11. Training loss:  0.355. Validation loss:  0.396. Validation accuracy 78.523%\n",
      "Epoch12. Training loss:  0.350. Validation loss:  0.392. Validation accuracy 78.523%\n",
      "Epoch13. Training loss:  0.346. Validation loss:  0.388. Validation accuracy 78.523%\n",
      "Epoch14. Training loss:  0.342. Validation loss:  0.384. Validation accuracy 78.523%\n",
      "Epoch15. Training loss:  0.339. Validation loss:  0.382. Validation accuracy 78.747%\n",
      "Epoch16. Training loss:  0.336. Validation loss:  0.379. Validation accuracy 78.523%\n",
      "Epoch17. Training loss:  0.334. Validation loss:  0.377. Validation accuracy 78.747%\n",
      "Epoch18. Training loss:  0.332. Validation loss:  0.375. Validation accuracy 78.747%\n",
      "Epoch19. Training loss:  0.330. Validation loss:  0.374. Validation accuracy 79.195%\n",
      "Epoch20. Training loss:  0.329. Validation loss:  0.372. Validation accuracy 79.642%\n",
      "Epoch21. Training loss:  0.328. Validation loss:  0.371. Validation accuracy 80.089%\n",
      "Epoch22. Training loss:  0.326. Validation loss:  0.370. Validation accuracy 80.313%\n",
      "Epoch23. Training loss:  0.325. Validation loss:  0.369. Validation accuracy 80.313%\n",
      "Epoch24. Training loss:  0.324. Validation loss:  0.368. Validation accuracy 80.313%\n",
      "Epoch25. Training loss:  0.323. Validation loss:  0.367. Validation accuracy 80.537%\n",
      "Epoch26. Training loss:  0.322. Validation loss:  0.366. Validation accuracy 80.313%\n",
      "Epoch27. Training loss:  0.321. Validation loss:  0.365. Validation accuracy 80.313%\n",
      "Epoch28. Training loss:  0.321. Validation loss:  0.365. Validation accuracy 80.313%\n",
      "Epoch29. Training loss:  0.320. Validation loss:  0.364. Validation accuracy 80.089%\n",
      "Epoch30. Training loss:  0.319. Validation loss:  0.363. Validation accuracy 79.866%\n",
      "Epoch31. Training loss:  0.319. Validation loss:  0.363. Validation accuracy 80.089%\n",
      "Epoch32. Training loss:  0.318. Validation loss:  0.362. Validation accuracy 79.866%\n",
      "Epoch33. Training loss:  0.318. Validation loss:  0.361. Validation accuracy 79.866%\n",
      "Epoch34. Training loss:  0.317. Validation loss:  0.361. Validation accuracy 79.866%\n",
      "Epoch35. Training loss:  0.317. Validation loss:  0.360. Validation accuracy 79.866%\n",
      "Epoch36. Training loss:  0.316. Validation loss:  0.360. Validation accuracy 80.089%\n",
      "Epoch37. Training loss:  0.316. Validation loss:  0.359. Validation accuracy 79.866%\n",
      "Epoch38. Training loss:  0.315. Validation loss:  0.359. Validation accuracy 79.866%\n",
      "Epoch39. Training loss:  0.315. Validation loss:  0.359. Validation accuracy 79.866%\n",
      "Epoch40. Training loss:  0.314. Validation loss:  0.358. Validation accuracy 80.313%\n",
      "Epoch41. Training loss:  0.314. Validation loss:  0.358. Validation accuracy 80.089%\n",
      "End of training\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "\n",
    "tf.compat.v1.reset_default_graph()#It will let us train our model from scratch. Simply by rerunning the cell.\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.int32, [None, output_size])\n",
    "\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [output_size])\n",
    "\n",
    "#outputs_3 = tf.nn.relu(tf.matmul(outputs_2, weights_3) + biases_3)\n",
    "\n",
    "#weights_4 = tf.get_variable(\"weights_4\", [hidden_layer_size, output_size])\n",
    "#biases_4 = tf.get_variable(\"biases_4\", [output_size])\n",
    "\n",
    "outputs = tf.matmul(outputs_2, weights_3 + biases_3)\n",
    "\n",
    "#tf.nn.softmax_cross_entropy_with_logits()-> is a function that applies a softmax activation and calculates a cross entropy loss simultaniously.\n",
    "#Logits here have a meaning of unscaled probability\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "mean_loss = tf.reduce_mean(loss) #is a method which find the mean of the elements of a tensor across a dimension.\n",
    "\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "out_equals_target = tf.equal(tf.arg_max(outputs, 1), tf.arg_max(targets, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32)) #cast-> changes the data type\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "initializer = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(initializer)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "max_epochs = 50\n",
    "\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "train_data = Audiobooks_Data_Reader('train', batch_size)\n",
    "validation_data = Audiobooks_Data_Reader('validation')\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    curr_epoch_loss = 0.\n",
    "    #Training\n",
    "    for input_batch, target_batch in train_data:\n",
    "        #we optimize the mean loss by feeding the algo with data from train data in batched.\n",
    "        _, batch_loss = sess.run([optimize, mean_loss],\n",
    "                                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        curr_epoch_loss += batch_loss #Record the sum of all batch losses\n",
    "\n",
    "    curr_epoch_loss /= train_data.batch_count\n",
    "\n",
    "    #Validation\n",
    "    validation_loss = 0.\n",
    "    validation_accuracy = 0.\n",
    "    for input_batch, target_batch in validation_data:\n",
    "        #forward propogate\n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n",
    "                                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    print('Epoch' + str(epoch_counter+1)+\n",
    "         '. Training loss: ' + '{0: .3f}'.format(curr_epoch_loss)+\n",
    "         '. Validation loss: ' + '{0: .3f}'.format(validation_loss)+\n",
    "         '. Validation accuracy' + '{0: .3f}'.format(validation_accuracy * 100.)+'%')\n",
    "\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "\n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "print('End of training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ab688-361a-495a-ba81-053d1ac238bb",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "dd0dc463-6844-4f42-a73d-a8d7a408eab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  81.92%\n"
     ]
    }
   ],
   "source": [
    "test_data = Audiobooks_Data_Reader('test')\n",
    "\n",
    "for input_batch, target_batch in test_data:\n",
    "    test_accuracy = sess.run([accuracy],\n",
    "                            feed_dict = {inputs: input_batch, targets: target_batch})\n",
    "\n",
    "test_accuracy_percentage = test_accuracy[0]*100\n",
    "print('Test accuracy: ' + '{0: .2f}'.format(test_accuracy_percentage)+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
